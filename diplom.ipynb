{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyOH0AAWb0j+7DsIBRroavs0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#@title requirements {display-mode:\"form\"}\n","!pip install -q supervision==0.1.0 ultralytics\n","\n","!git clone https://github.com/ifzhang/ByteTrack.git\n","%cd ByteTrack\n","\n","# workaround related to https://github.com/roboflow/notebooks/issues/80\n","!sed -i 's/onnx==1.8.1/onnx==1.9.0/g' requirements.txt\n","\n","!pip3 install -q -r requirements.txt\n","!python3 setup.py -q develop\n","!pip install -q cython_bbox\n","!pip install -q onemetric\n","# workaround related to https://github.com/roboflow/notebooks/issues/112 and https://github.com/roboflow/notebooks/issues/106\n","!pip install -q loguru lap thop\n","\n","# %cd ..\n","\n","from IPython import display\n","display.clear_output()\n","\n","\n","import ByteTrack.yolox as yolox\n","print(\"yolox.__version__:\", yolox.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tdUVXWsY924U","executionInfo":{"status":"ok","timestamp":1709303642562,"user_tz":-180,"elapsed":71123,"user":{"displayName":"kin","userId":"00217082230292738154"}},"outputId":"1df6c6aa-3c7e-4c78-d1f9-9dd786d71a25"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["yolox.__version__: 0.1.0\n"]}]},{"cell_type":"markdown","source":["# utils"],"metadata":{"id":"9i4adYGgDcP7"}},{"cell_type":"code","source":["#@title yolov8_model {display-mode:\"form\"}\n","from ultralytics import YOLO\n","\n","\n","MODEL = \"/content/best1280.pt\"\n","model = YOLO(MODEL)\n","model.fuse()\n","\n","\n","CLASS_NAMES_DICT = model.names\n","CLASS_ID = [0, 1, 2, 3, 4, 5, 6, 7]\n","\n","CLASS_ID_BY_NAME = {\n","    \"chair\": 0,\n","    \"person\": 1,\n","    \"interactive whiteboard\": 2,\n","    \"keyboard\": 3,\n","    \"laptop\": 4,\n","    \"monitor\": 5,\n","    \"pc\": 6,\n","    \"table\": 7,\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SxqvwvpqDkai","executionInfo":{"status":"ok","timestamp":1709303643467,"user_tz":-180,"elapsed":907,"user":{"displayName":"kin","userId":"00217082230292738154"}},"outputId":"7094b56a-83d1-481c-b936-88f8f32a7515"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Model summary (fused): 168 layers, 3007208 parameters, 0 gradients, 8.1 GFLOPs\n"]}]},{"cell_type":"code","source":["#@title line_counter {display-mode:\"form\"}\n","from supervision.tools.detections import Detections, BoxAnnotator\n","from supervision.geometry.dataclasses import Point, Rect, Vector\n","from typing import List, Dict\n","import numpy as np\n","from supervision.draw.color import Color\n","from supervision.tools.line_counter import LineCounter, LineCounterAnnotator\n","import cv2\n","\n","\n","class CustomLineCounter:\n","    def __init__(self, start: Point, end: Point, classes: List):\n","        self.vector = Vector(start=start, end=end)\n","        self.tracker_state: Dict[str, bool] = {}\n","        self.start = start\n","        self.end = end\n","        self.result_dict = {\n","            # int(class_id): {\"in_count\": int(0), \"out_count\": int(0)}\n","            int(class_id): {\"in\": [], \"out\": []}\n","            for class_id in classes\n","        }\n","\n","\n","    def update(self, detections: Detections):\n","        for id in detections.class_id:\n","            mask = np.array(\n","                [class_id in [int(id)] for class_id in detections.class_id], dtype=bool\n","            )\n","            filtereddet = detections.filter(mask=mask, inplace=False)\n","\n","            for xyxy, confidence, class_id, tracker_id in filtereddet:\n","                # handle detections with no tracker_id\n","                if tracker_id is None:\n","                    continue\n","\n","                # we check if all four anchors of bbox are on the same side of vector\n","                x1, y1, x2, y2 = xyxy\n","                anchors = [\n","                    Point(x=x1, y=y1),\n","                    Point(x=x1, y=y2),\n","                    Point(x=x2, y=y1),\n","                    Point(x=x2, y=y2),\n","                ]\n","                triggers = [self.vector.is_in(point=anchor) for anchor in anchors]\n","\n","                # detection is partially in and partially out\n","                if len(set(triggers)) == 2:\n","                    continue\n","\n","                tracker_state = triggers[0]\n","                # handle new detection\n","                if tracker_id not in self.tracker_state:\n","                    self.tracker_state[tracker_id] = tracker_state\n","                    continue\n","\n","                # handle detection on the same side of the line\n","                if self.tracker_state.get(tracker_id) == tracker_state:\n","                    continue\n","\n","                self.tracker_state[tracker_id] = tracker_state\n","                if tracker_state:\n","                    # self.result_dict[int(id)][\"in_count\"] += 1\n","                    self.result_dict[int(id)][\"in\"].append(self.camera.get_time_now())\n","                else:\n","                    # self.result_dict[int(id)][\"out_count\"] += 1\n","                    self.result_dict[int(id)][\"out\"].append(self.camera.get_time_now())\n","\n","    def get_result_dict(self) -> dict:\n","        return self.result_dict\n","\n","\n","class CustomLineCounterAnnotator:\n","    def __init__(\n","        self,\n","        thickness: float = 2,\n","        color: Color = Color.white(),\n","        text_thickness: float = 2,\n","        text_color: Color = Color.black(),\n","        text_scale: float = 0.5,\n","        text_offset: float = 1.5,\n","        text_padding: int = 10,\n","        class_name_dict={},\n","        video_info=[],\n","    ):\n","        self.thickness: float = thickness\n","        self.color: Color = color\n","        self.text_thickness: float = text_thickness\n","        self.text_color: Color = text_color\n","        self.text_scale: float = text_scale\n","        self.text_offset: float = text_offset\n","        self.text_padding: int = text_padding\n","        self.class_name_dict = class_name_dict\n","        self.video_info = video_info\n","\n","    def annotate(self, frame: np.ndarray, line_counter: LineCounter) -> np.ndarray:\n","        cv2.line(\n","            frame,\n","            line_counter.vector.start.as_xy_int_tuple(),\n","            line_counter.vector.end.as_xy_int_tuple(),\n","            self.color.as_bgr(),\n","            self.thickness,\n","            lineType=cv2.LINE_AA,\n","            shift=0,\n","        )\n","        cv2.circle(\n","            frame,\n","            line_counter.vector.start.as_xy_int_tuple(),\n","            radius=5,\n","            color=self.text_color.as_bgr(),\n","            thickness=-1,\n","            lineType=cv2.LINE_AA,\n","        )\n","        cv2.circle(\n","            frame,\n","            line_counter.vector.end.as_xy_int_tuple(),\n","            radius=5,\n","            color=self.text_color.as_bgr(),\n","            thickness=-1,\n","            lineType=cv2.LINE_AA,\n","        )\n","\n","        report = \"\"\n","        for key in line_counter.result_dict:\n","            class_name = CLASS_NAMES_DICT[key]\n","            in_count = line_counter.result_dict[key][\"in_count\"]\n","            out_count = line_counter.result_dict[key][\"out_count\"]\n","            report += f\" | {class_name}: in {in_count} out {out_count}\"\n","        report += \" |\"\n","\n","        (report_width, report_height), _ = cv2.getTextSize(\n","            report, cv2.FONT_HERSHEY_SIMPLEX, self.text_scale, self.text_thickness\n","        )\n","\n","        report_x = int(((self.video_info.width) - report_width) / 2)\n","        report_y = int((150 + report_height) / 2 - self.text_offset * report_height)\n","\n","        report_background_rect = Rect(\n","            x=report_x,\n","            y=report_y - report_height,\n","            width=report_width,\n","            height=report_height,\n","        ).pad(padding=self.text_padding)\n","\n","        cv2.rectangle(\n","            frame,\n","            report_background_rect.top_left.as_xy_int_tuple(),\n","            report_background_rect.bottom_right.as_xy_int_tuple(),\n","            self.color.as_bgr(),\n","            -1,\n","        )\n","\n","        cv2.putText(\n","            frame,\n","            report,\n","            (report_x, report_y),\n","            cv2.FONT_HERSHEY_SIMPLEX,\n","            self.text_scale,\n","            self.text_color.as_bgr(),\n","            self.text_thickness,\n","            cv2.LINE_AA,\n","        )"],"metadata":{"id":"dR2kTEAQDsXh","executionInfo":{"status":"ok","timestamp":1709303643467,"user_tz":-180,"elapsed":2,"user":{"displayName":"kin","userId":"00217082230292738154"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#@title camera {display-mode:\"form\"}\n","import datetime\n","\n","\n","class Camera:\n","    def __init__(self, id: int, aud: int, line_counter, line_counter_annotator, video_path: str):\n","        self.id = id\n","        self.aud = aud\n","        self.line_counter = line_counter\n","        self.line_counter_annotator = line_counter_annotator\n","        self.video_path = video_path\n","\n","    def get_time_now(self) -> datetime.datetime:\n","        pass\n","\n","    def track_video(self):\n","        # create BYTETracker instance\n","        byte_tracker = BYTETracker(BYTETrackerArgs())\n","        # create VideoInfo instance\n","        video_info = VideoInfo.from_video_path(self.video_path)\n","        # create frame generator\n","        generator = get_video_frames_generator(self.video_path)\n","        # create LineCounter instance\n","        line_counter = CustomLineCounter(start=self.line_counter.start, end=self.line_counter.end, classes=CLASS_ID)\n","        # create instance of BoxAnnotator and LineCounterAnnotator\n","        box_annotator = BoxAnnotator(\n","            color=ColorPalette(), thickness=4, text_thickness=4, text_scale=2\n","        )\n","        line_annotator = CustomLineCounterAnnotator(\n","            thickness=4,\n","            text_thickness=1,\n","            text_scale=0.5,\n","            class_name_dict=CLASS_NAMES_DICT,\n","            video_info=video_info,\n","        )\n","\n","        # open target video file\n","        with VideoSink(self.video_path, video_info) as sink:\n","            # loop over video frames\n","            for frame in tqdm(generator, total=video_info.total_frames):\n","                # model prediction on single frame and conversion to supervision Detections\n","                results = model(frame)\n","                detections = Detections(\n","                    xyxy=results[0].boxes.xyxy.cpu().numpy(),\n","                    confidence=results[0].boxes.conf.cpu().numpy(),\n","                    class_id=results[0].boxes.cls.cpu().numpy().astype(int),\n","                )\n","\n","                # filtering out detections with unwanted classes\n","                mask = np.array(\n","                    [class_id in CLASS_ID for class_id in detections.class_id], dtype=bool\n","                )\n","                detections.filter(mask=mask, inplace=True)\n","\n","                # tracking detections\n","                tracks = byte_tracker.update(\n","                    output_results=detections2boxes(detections=detections),\n","                    img_info=frame.shape,\n","                    img_size=frame.shape,\n","                )\n","\n","                tracker_id = match_detections_with_tracks(\n","                    detections=detections, tracks=tracks\n","                )\n","\n","                detections.tracker_id = np.array(tracker_id)\n","\n","                # filtering out detections without trackers\n","                mask = np.array(\n","                    [tracker_id is not None for tracker_id in detections.tracker_id],\n","                    dtype=bool,\n","                )\n","                detections.filter(mask=mask, inplace=True)\n","                # format custom labels\n","                labels = [\n","                    f\"id{tracker_id} {CLASS_NAMES_DICT[class_id]} {confidence:0.2f}\"\n","                    for _, confidence, class_id, tracker_id in detections\n","                ]\n","                frame = box_annotator.annotate(\n","                    frame=frame, detections=detections, labels=labels\n","                )\n","\n","                line_counter.update(detections=detections)\n","                line_annotator.annotate(frame=frame, line_counter=self.line_counter)\n","\n","                sink.write_frame(frame)\n","        return line_counter.result_dict\n","\n","    def get_tracker_info(\n","        self,\n","    ) -> dict:\n","        return self.line_counter.get_result_dict()"],"metadata":{"id":"m5TEd-43D1WC","executionInfo":{"status":"ok","timestamp":1709303643467,"user_tz":-180,"elapsed":2,"user":{"displayName":"kin","userId":"00217082230292738154"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#@title tracker {display-mode:\"form\"}\n","import cv2\n","import matplotlib.pyplot as plt\n","import sys\n","import supervision\n","from supervision.draw.color import ColorPalette\n","from supervision.geometry.dataclasses import Point\n","from supervision.video.dataclasses import VideoInfo\n","from supervision.video.source import get_video_frames_generator\n","from supervision.video.sink import VideoSink\n","from supervision.notebook.utils import show_frame_in_notebook\n","from supervision.tools.detections import Detections, BoxAnnotator\n","from supervision.tools.line_counter import LineCounter, LineCounterAnnotator\n","from typing import List\n","import numpy as np\n","from tqdm.notebook import tqdm\n","from yolox.tracker.byte_tracker import BYTETracker, STrack\n","from onemetric.cv.utils.iou import box_iou_batch\n","from dataclasses import dataclass\n","from typing import Dict\n","import cv2\n","import numpy as np\n","from supervision.draw.color import Color\n","from supervision.geometry.dataclasses import Point, Rect, Vector\n","from supervision.tools.detections import Detections\n","\n","\n","class VideoProcessor:\n","    def __init__(\n","        self,\n","        video_path,\n","    ):\n","        self.video_path = video_path\n","        self.video = cv2.VideoCapture(video_path)\n","        self.width = int(self.video.get(cv2.CAP_PROP_FRAME_WIDTH))\n","        self.height = int(self.video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","    def get_frame_by_order(self, frame_number):\n","        self.video.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n","        ret, frame = self.video.read()\n","        if ret:\n","            return frame\n","        else:\n","            return None\n","\n","    def get_width_height(self):\n","        return self.width, self.height\n","\n","\n","@dataclass(frozen=True)\n","class BYTETrackerArgs:\n","    track_thresh: float = 0.25\n","    track_buffer: int = 30\n","    match_thresh: float = 0.8\n","    aspect_ratio_thresh: float = 3.0\n","    min_box_area: float = 1.0\n","    mot20: bool = False\n","\n","\n","# converts Detections into format that can be consumed by match_detections_with_tracks function\n","def detections2boxes(detections: Detections) -> np.ndarray:\n","    return np.hstack((detections.xyxy, detections.confidence[:, np.newaxis]))\n","\n","\n","# converts List[STrack] into format that can be consumed by match_detections_with_tracks function\n","def tracks2boxes(tracks: List[STrack]) -> np.ndarray:\n","    return np.array([track.tlbr for track in tracks], dtype=float)\n","\n","\n","# matches our bounding boxes with predictions\n","def match_detections_with_tracks(\n","    detections: Detections, tracks: List[STrack]\n",") -> Detections:\n","    if not np.any(detections.xyxy) or len(tracks) == 0:\n","        return np.empty((0,))\n","\n","    tracks_boxes = tracks2boxes(tracks=tracks)\n","    iou = box_iou_batch(tracks_boxes, detections.xyxy)\n","    track2detection = np.argmax(iou, axis=1)\n","\n","    tracker_ids = [None] * len(detections)\n","\n","    for tracker_index, detection_index in enumerate(track2detection):\n","        if iou[tracker_index, detection_index] != 0:\n","            tracker_ids[detection_index] = tracks[tracker_index].track_id\n","\n","    return tracker_ids\n","\n","\n","def track_video(\n","    SOURCE_VIDEO_PATH,\n","    LINE_START,\n","    LINE_END,\n","    CLASS_ID,\n","    TARGET_VIDEO_PATH,\n","    CLASS_NAMES_DICT,\n","    model,\n","):\n","    # create BYTETracker instance\n","    byte_tracker = BYTETracker(BYTETrackerArgs())\n","    # create VideoInfo instance\n","    video_info = VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n","    # create frame generator\n","    generator = get_video_frames_generator(SOURCE_VIDEO_PATH)\n","    # create LineCounter instance\n","    line_counter = CustomLineCounter(start=LINE_START, end=LINE_END, classes=CLASS_ID)\n","    # create instance of BoxAnnotator and LineCounterAnnotator\n","    box_annotator = BoxAnnotator(\n","        color=ColorPalette(), thickness=4, text_thickness=4, text_scale=2\n","    )\n","    line_annotator = CustomLineCounterAnnotator(\n","        thickness=4,\n","        text_thickness=1,\n","        text_scale=0.5,\n","        class_name_dict=CLASS_NAMES_DICT,\n","        video_info=video_info,\n","    )\n","\n","    # open target video file\n","    with VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n","        # loop over video frames\n","        for frame in tqdm(generator, total=video_info.total_frames):\n","            # model prediction on single frame and conversion to supervision Detections\n","            results = model(frame)\n","            detections = Detections(\n","                xyxy=results[0].boxes.xyxy.cpu().numpy(),\n","                confidence=results[0].boxes.conf.cpu().numpy(),\n","                class_id=results[0].boxes.cls.cpu().numpy().astype(int),\n","            )\n","\n","            # filtering out detections with unwanted classes\n","            mask = np.array(\n","                [class_id in CLASS_ID for class_id in detections.class_id], dtype=bool\n","            )\n","            detections.filter(mask=mask, inplace=True)\n","\n","            # tracking detections\n","            tracks = byte_tracker.update(\n","                output_results=detections2boxes(detections=detections),\n","                img_info=frame.shape,\n","                img_size=frame.shape,\n","            )\n","\n","            tracker_id = match_detections_with_tracks(\n","                detections=detections, tracks=tracks\n","            )\n","\n","            detections.tracker_id = np.array(tracker_id)\n","\n","            # filtering out detections without trackers\n","            mask = np.array(\n","                [tracker_id is not None for tracker_id in detections.tracker_id],\n","                dtype=bool,\n","            )\n","            detections.filter(mask=mask, inplace=True)\n","            # format custom labels\n","            labels = [\n","                f\"id{tracker_id} {CLASS_NAMES_DICT[class_id]} {confidence:0.2f}\"\n","                for _, confidence, class_id, tracker_id in detections\n","            ]\n","            frame = box_annotator.annotate(\n","                frame=frame, detections=detections, labels=labels\n","            )\n","\n","            line_counter.update(detections=detections)\n","            line_annotator.annotate(frame=frame, line_counter=line_counter)\n","\n","            sink.write_frame(frame)\n","    return line_counter.result_dict"],"metadata":{"id":"ifs-6RcIEAP4","executionInfo":{"status":"ok","timestamp":1709303643923,"user_tz":-180,"elapsed":458,"user":{"displayName":"kin","userId":"00217082230292738154"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# _"],"metadata":{"id":"H9PxEHS2EGCf"}},{"cell_type":"code","source":["coords = [[1100, 230],[1200, 750]]\n","video_path = '/content/PXL_20240301_123203882.mp4'\n","video_info = VideoInfo.from_video_path(video_path)\n","\n","camera = Camera(\n","    id=1,\n","    aud=21,\n","    line_counter=CustomLineCounter(\n","        Point(0, 0),\n","        Point(0, 0),\n","        classes=CLASS_ID\n","    ),\n","    line_counter_annotator=CustomLineCounterAnnotator(\n","        class_name_dict=CLASS_NAMES_DICT,\n","        thickness=4,\n","        text_thickness=1,\n","        text_scale=0.5,\n","        video_info=video_info\n","    ),\n","    video_path = video_path\n",")"],"metadata":{"id":"oSrskRgvEgbq","colab":{"base_uri":"https://localhost:8080/","height":332},"executionInfo":{"status":"error","timestamp":1709303643923,"user_tz":-180,"elapsed":5,"user":{"displayName":"kin","userId":"00217082230292738154"}},"outputId":"c6db700d-d68c-4119-8671-3e008a4a18cb"},"execution_count":6,"outputs":[{"output_type":"error","ename":"Exception","evalue":"Could not open video at /content/PXL_20240301_123203882.mp4","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-ef510d357bf2>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m230\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m750\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvideo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/PXL_20240301_123203882.mp4'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvideo_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoInfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_video_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m camera = Camera(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/supervision/video/dataclasses.py\u001b[0m in \u001b[0;36mfrom_video_path\u001b[0;34m(cls, video_path)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not open video at {video_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCAP_PROP_FRAME_WIDTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: Could not open video at /content/PXL_20240301_123203882.mp4"]}]},{"cell_type":"code","source":["camera.track_video()"],"metadata":{"id":"7zqy068gCkmp","executionInfo":{"status":"aborted","timestamp":1709303643923,"user_tz":-180,"elapsed":4,"user":{"displayName":"kin","userId":"00217082230292738154"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%conda"],"metadata":{"id":"jHCaLov18j_x","executionInfo":{"status":"error","timestamp":1709312221111,"user_tz":-180,"elapsed":5,"user":{"displayName":"kin","userId":"00217082230292738154"}},"outputId":"69735973-a6cd-462a-ddd3-367fd98c85b8","colab":{"base_uri":"https://localhost:8080/","height":321}},"execution_count":2,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"The python kernel does not appear to be a conda environment.  Please use ``%pip install`` instead.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-64df76c9ebfc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conda'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-100>\u001b[0m in \u001b[0;36mconda\u001b[0;34m(self, line)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/packaging.py\u001b[0m in \u001b[0;36mconda\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \"\"\"\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_conda_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             raise ValueError(\"The python kernel does not appear to be a conda environment.  \"\n\u001b[0m\u001b[1;32m     87\u001b[0m                              \"Please use ``%pip install`` instead.\")\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: The python kernel does not appear to be a conda environment.  Please use ``%pip install`` instead."]}]}]}